\section{Perceptrons}

\begin{itemize}
  \item Modell eines künstlichen Neurons
  \item Vorgänger der \emph{Sigmoid Neurons}, die in heutigen modernen neuronalen Netzen benutzt werden
\end{itemize}

\begin{center}
\begin{tikzpicture}[->, shorten >= 2pt]
  \tikzstyle{neuron}=[circle,draw,minimum size=24pt, inner sep=0pt]

  \foreach \name / \y in {1,...,3}
    \node (I-\name) at (0,-\y) {$x_{\name}$};

  \foreach \name / \y in {1,...,1}
    \node[neuron] (H1-\name) at (\layersep,-\y-1) {};

  \node (O) at (2*\layersep, -2) {Output};

  \foreach \source in {1,...,3}
    \foreach \dest in {1,...,1}
      \path (I-\source) edge (H1-\dest);

  \foreach \source in {1,...,1}
    \path (H1-\source) edge (O);
\end{tikzpicture}
\end{center}

\begin{itemize}
  \item \underline{Eingaben:} $x_1, x_2, \ldots, x_n \in \lbrace 0, 1 \rbrace$
  \item \underline{\emph{Weights}:} $w_1, w_2, \ldots, w_n \in \mathbb{R}$ für jede Eingabe $x_1$, der die jeweilige Eingabe gewichtet
  \item $\text{Output} = \begin{cases}
      0, & \text{wenn }\sum_j w_j x_j \leq \text{Treshold, wobei Treshold} \in \mathbb{R}\\
    1, & \text{sonst}
  \end{cases}$
  \item \textbf{vereinfachte Schreibweise:}
  \begin{itemize}
    \item $\sum_j w_j x_j = w \cdot x$, wobei $w$ und $x$ nun Vektoren beschreiben, dessen Komponenten die Gewichte und Eingaben sind
    \item ziehe den Treshold auf die andere Seite der Ungleichung (\emph{Bias}: $b = - \text{Treshold}$)
    \item $\Rightarrow$ Bias beschreibt, wie einfach es ist ein Perceptron auf $1$ zu bringen
    \item $\text{Output} = \begin{cases}
      0, & \text{wenn }w \cdot x + b \leq 0\\
      1, & \text{sonst}
    \end{cases}$
  \end{itemize}
\end{itemize}

\begin{itemize}
  \item Mit Hilfe eines \emph{neuronalen Netzes} aus Perceptons können kompliziertere Entscheidungen getroffen werden:
\end{itemize}

\begin{center}
\begin{tikzpicture}[->, shorten >= 2pt]
  \tikzstyle{every pin edge}=[<-,shorten <= 4pt]
  \tikzstyle{neuron}=[circle,draw,minimum size=24pt, inner sep=0pt]

  \foreach \name / \y in {1,...,5}
    \node[neuron,pin=left:$x_{\y}$] (I-\name) at (0,-\y) {};

  \foreach \name / \y in {1,...,3}
    \node[neuron] (H1-\name) at (\layersep,-\y-1) {};

  \foreach \name / \y in {1,...,4}
    \path[yshift=-0.5cm]
      node[neuron] (H2-\name) at (2*\layersep,-\y) {};

  \node[neuron,pin=right:Output] (O) at (3*\layersep, -3) {};

  \foreach \source in {1,...,5}
    \foreach \dest in {1,...,3}
      \path (I-\source) edge (H1-\dest);

  \foreach \source in {1,...,3}
    \foreach \dest in {1,...,4}
      \path (H1-\source) edge (H2-\dest);

  \foreach \source in {1,...,4}
    \path (H2-\source) edge (O);
\end{tikzpicture}
\end{center}

\begin{itemize}
  \item neuronales Netz besteht aus drei Schichten: \emph{Input-Layer}, \emph{Hidden-Layer} und \emph{Output-Layer}
  \item \underline{Ziel:} bringe das Netz dazu zu lernen, d.h.\ ihre Weights und Bias-Werte anzupassen, sodass für jede Eingabe das erwartete Ergebnis erzielt wird
\end{itemize}
