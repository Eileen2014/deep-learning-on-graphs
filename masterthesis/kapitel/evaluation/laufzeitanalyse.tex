\section{Laufzeitanalyse}
\label{laufzeitanalyse}

Die Vorverarbeitung der Eingabedaten sowie das Training der neuronalen Netze erfolgte auf einem 3.5 GHz 6-Core Intel Xeon E5, 16GB DDR3 Arbeitsspeicher und einer Solid-State-Drive.
Die in dieser Arbeit präsentierten Laufzeiten wurden ausschließlich über Berechnungen auf dem Prozessor gewonnen.
Bei der Verwendung einer Grafikkarte stellen sich dabei Laufzeitgewinne in Bereichen, die auf die diese ausgelagert werden können, von $300 - 400 \%$ ein.

\paragraph{Vorverarbeitung}

Die Vorverarbeitung der Eingabedaten in eine geeignete Graphrepräsentation, das Berechnen der Merkmalsmatrix sowie zusätzliche Aufwände wie die Vergröberung des Graphen oder die Bestimmung der Receptive-Fields seiner Knoten, welche für die Eingabe in ein neuronales Netz notwendig sind, trägt einen entscheidenen Anteil an der Ausführungsgeschwindigkeit des Trainings und der Auswertung eines neuronalen Netzes bei.
Graphen sollen dabei bestmöglichst zur Laufzeit eines Trainings generiert werden können und möglichst die Ausführung des eigentlichen Trainings garnicht bis kaum beeinflussen.

Abbildung~\ref{fig:laufzeit_raeumlich_pipeline} illustriert die relative Laufzeitverteilung der einzelnen räumlichen Vorverarbeitungsschritte \bzgl{} aller Datensätze und Superpixelalgorithmen.
\input{tikz/laufzeit_raeumlich_pipeline}
Dabei stellt sich die Berechnung der eindeutigen Receptive-Fields einer groß gewählten Kotenauswahl als Flaschenhals in (fast) allen Datensätzen heraus, die die meiste Zeit in Anspruch nimmt.
Lediglich auf großen Bildern, wie sie in \gls{Pascal} enthalten sind, zeigt sich der Aufwand der Segmentierung als ebenso kostspielig.

Abbildung~\ref{fig:laufzeit_spektral_pipeline} veranschaulicht auf ähnliche Weise die Laufzeitverteilung der spektralen Vorverarbeitungsschritte.
\input{tikz/laufzeit_spektral_pipeline}
Anstelle der Receptive-Field-Berechnung entsteht hier ein Mehraufwand durch die Berechnung der Graphvergröberung.
Dieser Aufwand zeigt sich jedoch auf Bildern aus \gls{Pascal} als zu vernachlässigen, da die Laufzeit bei großen Bildern (fast) rein von der Laufzeit der Superpixelalgorithmen abhängig ist.

Tabelle~\ref{tab:laufzeit_raeumlich_spektral} stellt die absoluten Aufwände zur räumlichen sowie spektralen Vorverarbeitung eines Bildes aus allen Datensätze und Superpixelalgorithmen gegenüber.
\begin{table}[t]
\centering
\begin{tabular}{lrrrr}
  \toprule
  & \multicolumn{2}{c}{Räumlich [ms]} & \multicolumn{2}{c}{Spektral [ms]}\\
  \cmidrule{2-5}
  & \acs{SLIC} & \acs{QS} & \acs{SLIC} & \acs{QS}\\
  \midrule
  \acs{MNIST} & 30.59 & 24.00 & 20.32 & 19.40\\
  \acs{Cifar}-10 & 72.74 & 43.85 & 25.34 & 23.18\\
  \acs{Pascal} & 713.40 & 1080.78 & 372.79 & 782.56\\
  \bottomrule
\end{tabular}
\caption[Laufzeiten der räumlichen und spektralen Vorverarbeitung]{Laufzeiten der räumlichen sowie spektralen Vorverarbeitung für ein Bild der jeweiligen Datensätze und Superpixelalgorithmen basierend auf den Laufzeiten aus Abbildung~\ref{fig:laufzeit_raeumlich_pipeline} und~\ref{fig:laufzeit_spektral_pipeline}.
Die räumliche Vorverarbeitung ist aufgrund der einzelnen Receptive-Field-Berechnungen deutlich ineffizienter.}
\label{tab:laufzeit_raeumlich_spektral}
\end{table}
Wohingegen sich für Bilder mit kleinen Auflösungen aus den Datensätzen \gls{MNIST} und \gls{Cifar}-10 der Aufwand zur Vorverarbeitung in Grenzen hält, zeigen sich für ein Bild aus \gls{Pascal} die Kosten zur Vorverarbeitung als nicht zu unterschätzen.
Eine Vorverarbeitung zur Laufzeit ist auf diesem Datensatz daher (zurzeit) nicht zu empfehlen.
Weiterhin sei anzumerken, dass die spektralen Vorverarbeitungsschritte in jedem Versuchsaufbau entschieden schneller als die räumlichen berechnet werden können.

\paragraph{Training}

Tabelle~\ref{tab:train_laufzeit} zeigt die im Durschnitt ermittelten Laufzeiten des Trainings eines Batches der Größe $64$ für alle vorgestellten Netzarchitekturen.
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllrrrr}
  \toprule
  Datensatz & Superpixel & Verfahren & \multicolumn{4}{c}{Laufzeit [s]}\\
  \cmidrule{4-7}
   & & & vor Beginn & \multicolumn{3}{c}{zur Laufzeit}\\
  \cmidrule{5-7}
   & & & & Vorverarbeitung & Training & $\sum$\\
  \midrule
  \gls{MNIST} & \gls{SLIC} & \acs{RL}    & 0.04 & 1.23 & 0.04 & 1.27 \\
  \gls{MNIST} & \gls{SLIC} & \acs{SGCNN} & 0.36 & 0.62 & 0.33 & 0.95\\
  \gls{MNIST} & \gls{SLIC} & \acs{GCN}   & 0.20 & 0.74 & 0.21 & 0.95\\
  \gls{MNIST} & \gls{SLIC} & \acs{EGCNN} & 0.61 & 0.45 & 0.61 & 1.05\\
  \gls{MNIST} & \acs{QS}   & \acs{RL}    & 0.04 & 1.35 & 0.04 & 1.39\\
  \gls{MNIST} & \acs{QS}   & \acs{SGCNN} & 0.37 & 0.74 & 0.34 & 1.08\\
  \gls{MNIST} & \acs{QS}   & \acs{GCN}   & 0.21 & 0.87 & 0.21 & 1.08\\
  \gls{MNIST} & \acs{QS}   & \acs{EGCNN} & 0.63 & 0.54 & 0.63 & 1.17\\
  \midrule
  \gls{Cifar}-10 & \gls{SLIC} & \acs{SGCNN} & — & & & \\
  \gls{Cifar}-10 & \gls{SLIC} & \acs{GCN}   & — & 1.24 & 0.39 & 1.63 \\
  \gls{Cifar}-10 & \gls{SLIC} & \acs{EGCNN} & — & 0.44 & 2.00 & 2.44 \\
  \gls{Cifar}-10 & \acs{QS}   & \acs{SGCNN} & — & 0.43 & 1.95 & 2.38\\
  \gls{Cifar}-10 & \acs{QS}   & \acs{GCN}   & — & 1.09 & 0.36 & 1.45\\
  \gls{Cifar}-10 & \acs{QS}   & \acs{EGCNN} & — & & & \\
  \midrule
  \gls{Pascal} & \gls{SLIC} & \acs{EGCNN} & 4.15 & 23.05 & 4.17 & 27.22\\
  \gls{Pascal} & \acs{QS}   & \acs{EGCNN} & 5.28 & 51.32 & 5.46 & 56.78 \\
  \midrule
  \gls{MNIST}    & — & klassisch & 0.19 & \multicolumn{3}{c}{—}\\
  \gls{Cifar}-10 & — & klassisch & 0.48 & \multicolumn{3}{c}{—}\\
  \gls{Pascal}   & — & klassisch & & \multicolumn{3}{c}{—}\\
  \bottomrule
\end{tabular}}
\caption[Vergleich der Trainingslaufzeiten]{Vergleich der Laufzeiten des Trainings eines Batches der Größe 64 für alle vorgestellten Netzarchitekturen.}
\label{tab:train_laufzeit}
\end{table}
Die Eingabedaten wurden für \gls{MNIST} \bzw{} \gls{Pascal} aufgrund nicht benötigter Augmentierung \bzw{} der aufwändigen Berechnungen vorab gespeichert, wohingegen die Grapheingabedaten für \gls{Cifar}-10 zur Laufzeit generiert wurden.
Es zeigt sich, dass das Training auf Graphen im Allgemeinen deutlich langsamer als im Vergleich zu ausgereiften Bildimplementierungen ist.
Lediglich der Ansatz des räumlichen Lernens kann aufgrund seiner Verwendung der klassischen Faltungsoperation und bei einer Vorverarbeitung vor Beginn des Trainings schnelle Laufzeiten garantieren.

Zwischen den spektralen Faltungsansätzen zeigt sich das \acs{GCN} aufgrund seiner geringen Berechnungskomplexität als sehr effizient, leidet jedoch wie in Kapitel~\ref{ergebnisse} beschrieben an mangelnder Genauigkeit.
Das \acs{EGCNN} weist unter allen Ansätzen die höchste Berechnungskomplexität auf.
Dies ist im Vergleich mit dem \acs{SGCNN} insoweit zu erklären, dass dessen Netzarchitektur in der Regel tiefer ist und mehrere Faltungsschichten hintereinander verlangt, um an Informationen nicht direkt benachbarter Knoten zu gelangen.
Das \acs{SGCNN} erfordert dies nicht, da dessen Filtergröße in direktem Zusammenhang zur Größe der Faltung steht.

Es ist weiter bei einer Vorverarbeitung während des Trainings zu beobachten, dass die Aufwände für die Vorverarbeitung umso höher werden, umso schneller die eigentliche Lernprozedur vonstatten geht.
So zeigt sich \bspw{} bei \gls{Cifar}-10, dass das \acs{EGCNN} und \acs{SGCNN} in der Regel lediglich $\approx$ 0.4s auf die Vorverarbeitung warten müssen, wohingegen bei der effizienteren Faltung des \acs{GCN}s das Training pro Batch $\approx$ 1.1s pausiert.
Damit wird trotz Multithreading die Vorverarbeitung zur Laufzeit zum Flaschenhals des Trainings.

\paragraph{Vergleich \bzgl{} vorhandener Implementierungen}
\label{vergleich_laufzeit}

Obwohl die Vorverarbeitung das Training eines neuronalen Netzes zur Laufzeit behindern kann, so konnten bereits viele Vorverarbeitungsschritte im Laufe dieser Arbeit effizienter als ihre vorhandenen, quelloffenenen Implementierungen gestaltet werden.
So besitzt \bspw{} \texttt{skimage} eine Implementierung zur Adjazenzbestimmung einer Segmentierungsmaske und zur Extraktion der meisten in dieser Arbeit vorgestellten Merkmale.
Beide Verfahren konnten mit deutlichem Laufzeitgewinnn optimiert werden.
Der frei zugängliche Quelltext des spektralen Faltungsoperators (\acs{SGCNN}) besitzt ebenso eine Implementierung zur Vergröberung eines Graphen.
Dieser konnte, trotz der Berücksichtigung der Knotenpositionen und Generierung zweier Adjazenzmatrizen \gls{Adist} und \gls{Arad}, fast dreimal so schnell implementiert werden.
Abbildung~\ref{fig:laufzeit_vergleich} fasst die beschriebenen Laufzeitgewinne separat zusammen.
\input{tikz/laufzeit_vergleich}
