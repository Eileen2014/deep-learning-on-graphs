\section{Ergebnisse}
\label{ergebnisse}

Dieses Kapitel stellt die Ergebnisse einer ausgewählten Untermenge der in dieser Arbeit vorgestellten Graphrepräsentationen von Bildern, den Ansätzen zum Lernen auf diesen sowie den beschriebenen Datensätzen und Superpixelalgorithmen vor.
Zur Kenntlichmachung der einzelnen Ansätze erhält das räumliche Lernen auf Graphen dabei die Abkürzung \enquote{\acs{RL}} und das spektrale Lernen auf Graphen über den Tschebyschow-Polynomen \bzw{} dessen Einschränkung auf eine Filtergröße von Eins die Abkürzungen \enquote{\acs{SGCNN}} \bzw{} \enquote{\acs{GCN}}.
Der erweiterte Ansatz zum spektralen Lernen ist über \enquote{\acs{EGCNN}} gekennzeichnet.
Alle Faltungen des \acs{EGCNN}s wurden dabei mit einer Partitionsgröße von $8$ bei einem Grad von Eins implementiert, um ein \gls{CNN} mit einem $3 \times 3$ Filter zu simulieren.

Netzarchitekturen werden im Folgenden \bspw{} über ein Muster der Form \enquote{C32-P2-FC128} beschrieben und signalisieren eine Faltungsschicht, die auf 32 Merkmalskarten abbildet, gefolgt von einer Poolingschicht, welche zwei Knoten vereint, und danach mit einer vollverbundenen Schicht zu 128 Neuronen verknüpft wird.
Die für das spektrale Lernen bei dynamischen Eingabegrößen notwendige Durchschnittsbildung zwischen Faltungs- und vollverbundener Schicht wird als \enquote{Avg} gekennzeichnet (\vgl{} Kapitel~\ref{spektrale_netzarchitektur}).
Auf die explizite Erwähnung der Eingabe- und Ausgabeschichten der Netze wird übersichtshalber verzichtet.

\paragraph{\gls{MNIST}}

Für die Validierung der Faltungsansätze starten wir mit dem \gls{MNIST} Datensatz und der in Kapitel~\ref{gitter} beschriebenen Graphrepräsentation der Bilder über ein reguläres Gitter.
Damit kann jedes Bild über den selben Graphen dargestellt werden.
Insbesondere verfällt damit die Notwendigkeit der Durchschnittsbildung, da die Anzahl an Neuronen zu jedem Zeitpunkt klar definiert ist.
Für das räumliche Lernen wurden 196 Knoten mit Schrittweite 4 bei $\delta=1$ und einer Nachbarschaftsgröße von 25 besimmt (\vgl{} Kapitel~\ref{raeumliches_lernen}).
Die Graphvergröberung des spektralen Lernens wiederum generiert für die erste Schicht 976 Knoten ($28^2 = 784$ Pixelknoten und 192 Fakeknoten).
Die Anzahl der generierten Fakeknoten kann jedoch aufgrund der zufälligen Permutation variieren.
Tabelle~\ref{tab:train_mnist_gitter} fasst die Netzarchitekturen und dessen erreichte Genauigkeiten der einzelnen Ansätze zusammen.
\begin{table}[t]
\centering
\begin{tabular}{lclrr}
  \toprule
  Ansatz & \ma{W} & Architektur & Genauigkeit [\%]\\
  \midrule
  \acs{RL} & 25 & C64-FC1024 & 98.774 \\
  \acs{SGCNN} & 25 & C32-P4-C64-P4-FC1024 & 98.888\\
  \acs{GCN} & 1 & C32-C32-P4-C64-C64-P4-FC1024 & 96.675\\
  \acs{EGCNN} & 9 & C32-C32-P4-C64-C64-P4-FC1024 & 99.145\\
  \midrule
  klassisch & $5 \times 5$ & C32-P4-C64-P4-FC1024 & 99.189\\
  klassisch & $3 \times 3$ & C32-C32-P4-C64-C64-P4-FC1024 & 99.139\\
  \bottomrule
\end{tabular}
\caption[Testgenauigkeiten der \gls{MNIST}-Gitterrepräsentation]{Testgenauigkeiten eines Trainings auf einer \gls{MNIST}-Gitterrepräsentation nach 10 Epochen für die vorgestellten Ansätze inklusive äquivalenter Netzarchitekturen bei Benutzung der klassischen Faltungsoperation.
Die Spalte \ma{W} kennzeichnet die benutzten Filtergrößen der einzelnen Faltungsschichten.}
\label{tab:train_mnist_gitter}
\end{table}
Die Netzarchitekturen sind dabei an der von \texttt{tensorflow} vorgeschlagenen Architektur orientiert~\cite{tensorflow}.
Alle Netze wurden einheitlich mit einer Lernrate von $0.001$ trainiert.
Es wurde auf die für \gls{MNIST} eher untypische Augmentierung der Eingabedaten, die L2-Regularisierung sowie auf die Verminderung der Lernrate in Abhängigkeit zur Trainingsdauer ver\-zich\-tet~\cite{tensorflow}.
Es findet sich jedoch die Anwendung eines Dropouts von $0.5$ vor der Ausgabeschicht.
\citeauthor{Defferrard} benutzen zur Validierung des \acs{SGCNN}s eine ähnliche Netzarchitektur auf \gls{MNIST} und schlagen dazu eine Filtergröße von $25$ vor, sodass die Anzahl der Parameter mit der Anzahl der klassischen Faltung über ein $5 \times 5$ Fenster korrespondiert~\cite{Defferrard}.
Diese Faltung lässt sich im Kontext des spektralen Faltungsoperators auf einem Gitter der Größe $28 \times 28$ aber kaum noch als lokale Faltung verstehen, denn sie lässt Knoten in die Berechnung mit einfließen, die 25 Kanten weit vom Ursprungsknoten entfernt liegen, und betrachtet folglich außer bei Randknoten das globale Bild (\vgl{} Kapitel~\ref{spektraler_faltungsoperator}).
Eine lokalere Faltung lässt sich mit dem \acs{RL}, \acs{GCN} und \acs{EGCNN} gewinnen.
Für das \acs{GCN} zeigt sich dabei jedoch eine eindeutige Limitierung durch die geringe Filtergröße.
Die entwickelten Faltungsansätze \acs{RL} und \acs{EGCNN} beweisen sich aufgrund der Berücksichtigung ihrer Kantenausrichtungen als die bessere Alternative.
Insbesondere lässt sich die Äquivalenz des \acs{EGCNN}s \bzgl{} regulärer Gitter verifizieren.
Das räumliche Lernen zeigt sich aufgrund der Beschränkung auf eine Faltungsschicht, der eindimensionalen Knotenauswahl sowie der uneinheitlichen Nachbarschaftsanordnung an Randknoten als leicht schwächer.

Die in Tabelle~\ref{tab:train_mnist} vorgestellten Netzarchitekturen zeigen die Genauigkeiten des Trainings auf irregulären Dateneingaben, welche über die Superpixelalgorithmen \gls{SLIC} und \gls{QS} anhand der in Kapitel~\ref{datensaetze} vorgestellten Parameterwerte gewonnen wurden.
\begin{table}[t]
\centering
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \acs{SLIC} & \acs{QS}\\
  \midrule
  \acs{RL} & 25 & C64-FC1024 & 82.873 & 85.226 \\
  \acs{SGCNN} & 9 & C64-P4-C128-P4-Avg & 91.884 & 92.718 \\
  \acs{GCN} & 1 & C64-C64-P4-C128-C128-P4-Avg & 78.155 & 86.358 \\
  \acs{EGCNN} & 9 & C64-C64-P4-C128-C128-P4-Avg & 97.405 & 98.025 \\
  \bottomrule
\end{tabular}
\caption[Testgenauigkeiten der \gls{MNIST} Superpixelrepräsentationen]{Testgenauigkeiten eines Trainings auf dem \gls{MNIST} Datensatz nach 17 Epochen, dessen Bilder zuvor durch die Superpixelalgorithmen \gls{SLIC} und Quickshift in irreguläre Graphrepräsentationen gebracht werden.}
\label{tab:train_mnist}
\end{table}
Bei \acs{SGCNN} wurde dabei aufgrund der reduzierten Datenmenge die Filtergröße im Vergleich zum Vortest gedrosselt und besitzt im Vergleich zu den \acs{GCN}- \bzw{} \acs{EGCNN}-Netzarchitekturen nicht mehrere, aufeinanderfolgende Faltungsschichten.

Es zeigt sich, dass alle Netzansätze auf irregulären Daten gar nicht oder nur knapp an die Ergebnisse auf regulären Gittereingaben heranreichen.
Insbesondere offenbart der räumliche Faltungsansatz hier seine Schwächen, denn obwohl die Trainingsmenge erfolgreich gelernt wird, so lassen sich unbekannte Eingaben aufgrund der Anordnung zu eindimensionalen Knoten- sowie Nachbarschaftsmengen für irreguläre Graphen nur schwer bis gar nicht generalisieren.
Für alle Ansätze zeigt sich desweiteren die Quickshift-Segmentierung aufgrund ihrer überlegenden Segmentierung auf den Bildern des \gls{MNIST} Datensatz als die bessere Wahl (\vgl{} Abbildung~\ref{fig:mnist}).
Insbesondere erreicht der entwickelte spektrale Faltungsoperator über B-Spline-Kurven (\acs{EGCNN}) auch hier die eindeutig besten Resultate.

\paragraph{\gls{Cifar}-10}

Die Klassifikation auf dem \gls{Cifar}-10 Datensatz ist im Vergleich zu \gls{MNIST} eine anspruchsvollere Aufgabe, die insbesondere eine längere Trainingsdauer über weitaus mehr Epochen in Anspruch nimmt.
Wichtige Maßnahmen für qualitativ hochwertige Ergebnisse sind dabei insbesondere die in Kapitel~\ref{augmentierung} vorgestellten Augmentierungschritte der Eingabedaten~\cite{tensorflow}.
Es hat sich auch in der Evaluierung bewahrheitet, dass eine Augmentierung auf den Graphen schwächere Resultate als eine Augmentierung auf den Rohdaten des Bildes liefert und ein Genauigkeitsverlust von bis zu 5\% nach sich zieht.
Für alle Lernansätze wurden deshalb die Eingaben des \gls{Cifar}-10 Datensatz zur Laufzeit augmentiert und erst anschließend in eine Graph\-re\-prä\-sen\-ta\-tion transformiert.
Die Lernrate wurde dabei mit $0.001$ initialisiert und alle 6 Epochen exponentiell um $0.96$ verringert.
In den vollverbundenen Schichten, außer der Ausgabeschicht, besitzen die Gewichte \bzgl{} der L2-Regularisierung eine Dämpfungskonstante von $0.004$.
Vor der Ausgabeschicht sorgt die zusätzliche Anwendung eines Dropouts von $0.5$ dafür, ein Overfitting der Netze weiter zu vermeiden.
Tabelle~\ref{tab:train_cifar_10} fasst die Netzarchitekturen der einzelnen Ansätze und ihre erreichten Genauigkeiten nach rund 80 Epochen zusammen.
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \acs{SLIC} & \acs{QS}\\
  \midrule
  \acs{SGCNN} & 9 & C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & 66.814 & 67.748\\
  \acs{GCN} & 1 & C64-C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & 54.497 & 53.606\\
  \acs{EGCNN} & 9 & C64-C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & 74.218 & 75.230\\
  \midrule
  klassisch & $3 \times 3$ & C64-C64-P4-C128-P4-C256-P4-FC256-FC128 & \multicolumn{2}{c}{82.061} \\
  \bottomrule
\end{tabular}}
\caption[Testgenauigkeiten der \gls{Cifar}-10 Superpixelrepräsentationen]{Testgenauigkeiten eines Trainings auf den durch \gls{SLIC} und Quickshift zur Laufzeit generierten Graphrepräsentationen des \gls{Cifar}-10 Datensatz nach 80 Epochen im Vergleich zu einer ähnlichen klassischen Netzarchitektur auf den Bildrohdaten.}
\label{tab:train_cifar_10}
\end{table}
Alle spektralen Graphansätze auf den aus \gls{SLIC} und Quickshift generierten Graphrepräsentationen können nicht mit einer ähnlichen klassischen Netzarchitektur auf den Bildrohdaten konkurrieren.
Wohingegen jedoch die vorhandenen Ansätze \acs{SGCNN} und \acs{GCN} eine maximale Genauigkeit von rund 67\% erreichen, zeigt sich auch hier eine signfikante Verbesserung durch den neu entwickelten spektralen Faltungsoperator \acs{EGCNN}.
Er erreicht mit $75\%$ mittels der Quickshift-Segmentierung das beste Resultat.
Hochoptimierte Ansätze, bei denen noch weitaus mehr Augmentierungsschritte zum Einsatz kommen, erreichen auf \gls{Cifar}-10 jedoch bereits mehr als $90\%$ und so ist nicht auszuschließen, dass auch für \acs{EGCNN} weiterhin Potential nach oben besteht~\cite{fracmaxpool}.

Abbildung~\ref{fig:cifar_10_train} illustriert den Genauigkeitsverlauf des \gls{Cifar}-10 Trainings- und Validierungsdatensatzes in Abhängigkeit der vergangenen Epochen.
\input{tikz/cifar_10_train}
Wohingegen der klassische Ansatz recht schnell nach ungefähr 14 Epochen seine Maximalgenauigkeit erreicht und den Rest des Trainings damit verbringt, seine Trainingsdaten \enquote{überanzupassen}, ist zu beobachten, dass die Graphansätze weitaus langsamer und kontinuierlicher lernen.

\paragraph{\gls{Pascal}}

Für die Evaluierung der Ergebnisse auf dem \gls{Pascal} Datensatz wurde zum Vergleich die SqueezeNet-Architektur in Version 1.1 auf den Bildrohdaten zu Rate gezogen (\vgl{}~\cite{squeeze}).
Die Architektur des SqueezeNets gilt aufgrund bewährter Ergebnisse auf dem ImageNet Datensatz bei einer gleichzeitig extrem geringen Anzahl zu trainierender Parameter als \enquote{State-of-the-Art} in der Klassifizerung von Bildern mit recht hoher Auflösung~\cite{squeeze}.
Zur Verwendung wurden die Bilder dafür einheitlich auf die Größe $224 \times 224$ skaliert und mit schwarzen Bereichen an den horizontalen oder vertikalen Rändern je nach Ursprungsauflösung befüllt.
Für das Lernen auf Graphen ist diese Sklarierung der Bildeingabedaten nicht von Nöten.
Für die Netzarchitektur wurde dabei insbesondere im Vergleich zu vorherigen Testläufen auf die Verwendung mehrmaliger hintereinanderfolgender Faltungsschichten verzichtet.
Die Superpixel, die aus den Bildern von \gls{Pascal} generiert werden, sind bereits ausreichend groß und eine einzige Faltung auf diesen deckt bereits einen großen Bereich des Bildes ab.
Dabei findet das Training ausschließlich aus Laufzeit technischen Gründen auf vorverarbeiten Daten statt und erlaubt damit lediglich eine Augmentierung der Graphen zur Laufzeit (\vgl{} Kapitel~\ref{laufzeitanalyse}).
Es finden sich weiterhin Anwendungen der Dropout-Technik und der L2-Regularisierung in den vollverbundenen Schichten bei einer Lernrate von 0.0001 für das SqueezeNet und 0.001 bei den Graphansätzen.

Tabelle~\ref{tab:train_pascal} stellt die Ergebnisse des SqueezeNets mit den Ergebnissen auf irregulären Dateneingaben bei Benutzung des \acs{EGCNN}s gegenüber.
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \acs{SLIC} & \acs{QS}\\
  \midrule
  \acs{EGCNN} & 9 & C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & 54.473 & 54.516\\
  \midrule
  klassisch & — & SqueezeNet 1.1 & \multicolumn{2}{c}{54.731} \\
  \bottomrule
\end{tabular}}
\caption[Testgenauigkeiten der \gls{Pascal} Superpixelrepräsentationen]{Testgenauigkeiten eines Trainings auf den durch \gls{SLIC} und Quickshift vorab generierten Graphrepräsentationen des \gls{Pascal} Datensatz nach 70 Epochen im Vergleich zu der Verwendung des SqueezeNets in Version 1.1 auf den Bildrohdaten~\cite{squeeze}.}
\label{tab:train_pascal}
\end{table}
Es zeigt sich, dass auf \gls{Pascal} die Graphansätze durchaus mit klassischen Lösungen auf diesem Gebiet mithalten können.
Sie beweisen sich als lediglich marginal schwächer gegenüber der Benutzung einer ausgereiften Architektur wie dem SqueezeNet.
Die allgemein relativ schlechten Ergebnisse im Vergleich zu \gls{MNIST} und \gls{Cifar}-10 sind damit größtenteils dem Datensatz geschuldet, da dieser eine weitaus geringere Trainingsmenge sowie keine einander ausschließenden Bildklassen besitzt und seine Verwendung eher in der Lösung zu Problemen wie der Objekterkennung oder Bildsegmentierung sieht.
