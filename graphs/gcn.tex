\documentclass{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{ngerman}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{a4wide}
\usepackage{todonotes}
\usepackage{graphicx}

\title{GCN}

\begin{document}

\maketitle

\section{CNNs auf variierenden Größen}

Meistens werden Bilder für die Eingabe in ein CNN gecroppt oder gewarped, damit sie einer fixen Bildgröße (z.B. $244 \times 244$) entsprechen.
Dabei gehen aber Bildteile verloren oder werden geometrisch gestreckt, so dass sie nicht mehr naturgetreu sind.
Desweiteren helfen Bilder mit verschiedenen Größen dabei, skalierungsinvariant zu trainieren und reduzieren Overfitting.
Warum brauchen CNNs eine fixe Größe?
Ein CNN besteht aus zwei Teilen, Convolutional Layern und Fully-Connected Layern.
Die Convolutional Layer opeieren über einem beweglichen Fenster.
Convolutional Layer brauchen eigentlich keine fixe Größe und können Feature Maps beliebiger Größe generieren.
Das ist der Ursache geschuldet, dass wir immer nur einen Bildausschnitt betrachten und shared Weights and Biase haben.
Die Fully-Connected Layer allerdings haben diesen Vorteil nicht, sie brauchen stets eine feste Anzahl an Neuronen.
Beim Übergang zwischen Convolution und Fully-Connected kriegen wir deshalb Probleme.

Ein Layer zwischen diesen beiden Teilen mit der Aufgabe, eine beliebige Größe von Feature Maps auf eine feste Anzahl an Neuronen zu mappen, könnte dieses Problem beseitigen.
Dieser Layer wird auch SPP genannt (\emph{Spatial Pyramid Pooling}).

\subsection{SPP Layer}

\begin{figure}[h]
  \centering
  \includegraphics[width=.6\textwidth]{images/spp}
\end{figure}

Aus der letzten Convolution (z.B. $256$ Feature Maps, d.h. Shape \texttt{[-1, -1, 256]}) werden mehrere \emph{Spatial Bins} verschiedener Größen über \emph{Max-Pooling} berechnet.
In jedem Bin werden die Features Maps gepoolt mit Größe $kM$, wobei $M$ die Anzahl an Bins ist und $k$ die Anzahl an Feature Maps.

Angenommen wir haben eine Bildgröße von $224 \times 224$ und ein CNN, dass uns nach der Convolution Shapes der Form \texttt{[13, 13, 256]} liefert mit $a \times a = 13 \times 13$.
Sagen wir wir wollen $3$ 3 Bins aufbauen mit $l = 3$:
Dann berechnen sich die Pooling Windows mit $\lceil \frac{a}{n} \rceil$ und Strides mit $\lfloor \frac{a}{n} \rfloor$ mit $n = 1, 2, 3$.

Dann können wir z.B. 3 mal Max Pooling auf diesen Daten anwenden:

\begin{enumerate}
  \item $3 \times 3 \Rightarrow$ Size $5$, Stride $4$
  \item $2 \times 2 \Rightarrow$ Size $7$, Stride $6$
  \item $1 \times 1 \Rightarrow$ Size $13$ Stride $13$
\end{enumerate}

Dann ergibt dies:

\begin{equation}
  3 \times 3 \times 256 + 2 \times 2 \times 256 + 1 \times 1 \times 256 = 3584
\end{equation}

Diese Neuronen können wir nun mit unseren Fully Connected Neuronen verbinden.

Jetzt testen wir noch eine unterschiedliche Bildgröße von $180 \times 180$.
Dann hat das CNN nach der Convolution die Shape \texttt{[10, 10, 256]} mit $a \times a = 10 \times 10$.

\begin{enumerate}
  \item $3 \times 3 \Rightarrow$ Size $4$, Stride $3$
  \item $2 \times 2 \Rightarrow$ Size $5$, Stride $5$
  \item $1 \times 1 \Rightarrow$ Size $10$ Stride $10$
\end{enumerate}

Auch das ergibt:

\begin{equation}
  3 \times 3 \times 256 + 2 \times 2 \times 256 + 1 \times 1 \times 256 = 3584
\end{equation}

\section{Graph Convolutional Networks}

\begin{itemize}
  \item großes wissenschaftliches Problem, willkürliche Graphen in ein neuronales Netz zu füttern
  \item Bereich bisher dominiert von kernelbasierten Methoden, graphbasierter Regularisierung oder ähnlichem
  \item Es gibt eine Ansätze, der neuste ist ein \emph{spektraler}
  \item spektraler Ansatz gilt als langsam
  \item das ganze zählt zu dem Bereich des \emph{Semi-unsupervised Learning}, das bedeutet, dass wir einen Teil des Graphen gelabelt haben und basierend auf seiner Graphstruktur, den Rest labeln wollen.
\end{itemize}

\subsection{Definitionen}

\begin{itemize}
  \item Wir wollen eine Funktion von Merkmalen auf einem Graphen $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ lernen
  \item \textbf{Eingaben:}
  \begin{itemize}
    \item Eine Merkmalsbeschreibung $x_i$ für jeden Knoten $i$ dargestellt als eine Merkmalsmatrix $X = N \times D$, wobei $N$ Anzahl der Knoten und $D$ Anzahl der Merkmale
    \item Eine repräsentative Beschreibung einer Graphstruktur in Matrixform, normalerweise eine Adjazenzmatrix $A$
  \end{itemize}
  \item \textbf{Ausgabe}:
  \begin{itemize}
    \item eine Merkmalsbeschreibung $Z = N \times F$ für jeden Knoten, wobei $F$ Anzahl der Ausgabefeatures für \underline{einen} Knoten
  \end{itemize}
\end{itemize}

Jede Schicht des neuronalen Netzes kann über eine nicht-lineare Funktion

\begin{equation}
  H^{(l+1)} = f(H^{(l)}, A)
\end{equation}

beschrieben werden, wobei $H^{(0)} = X$ und $H^{(L)} = Z$ mit $L$ Anzahl der Schichten.

Die Propagationsregel ist dann zum Beispiel:

\begin{equation}
  f(H^{(l)}, A) = \sigma(AH^{(l)}W^{(l)})
\end{equation}

wobei, $\sigma$ eine nicht lineare Aktivierungsfunktion ist (z.B. \texttt{ReLU}) und $W^{(l)}$ eine Gewichtsmatrix für den $l$ten Layer.

Sagen wir $A$ ist eine $n \times n$ Matrix, die über mehrere Graphen hinweg variabel ist.
Graph $1$ hat zum Beispiel $n = 100$ Knoten, wohingegen Graph $2$ nur $n = 80$.
$H^{(l)}$ ist eine $n \times f_j$.
Die Anzahl an Features pro Layer sind unabhängig vom Graphen und immer gleich.
Dann ist $AH^{(l)}$ eine $n \times f_j$ Matrix.
$W^{(l)}$ ist eine $f_{j} \times f^{j+1}$ Matrix.
Damit ist die Gewichtsmatrix unabhängig der Knotenanzahl und kann auch für verschiedene große Graphen genutzt werden.
Die Gleichheit von $W^{(l)}$ liefert uns den Convolution Gedanken (Shared Weights).
Dann ist $H^{(l+1)}$ eine $n \times f_{j+1}$ Matrix.

Am Ende der Convolution hin zum Fully-Connected muss dann $n \times f_{j}$ gepoolt werden, damit es immer gleich viele Eingabeneuronen gibt (siehe SPP).

$A$ muss jedoch leicht modifziert werden, denn eine Multiplikation mit $A$ summiert alle Feature Vectors der lokalen Nachbarschaftsknoten auf, jedoch ohne den betrachteten Knoten (falls Knoten keine Kante zu sich selbst).
Wir können das fixen, in dem wir für jeden Knoten eine Kante sich zu selbst hinzufügen in dem wir $A$ mit der Identitätsmatrix addieren.

Desweiteren ist $A$ nicht normalisiert, das bedeutet, dass eine Multiplikation mit $A$ die Featurevectors komplett anders skaliert.
$A$ kann zum Beispiel normalisiert werden, in dem alle Reihen zu Eins aufsummiert werden.

\begin{equation}
  D_{ii} = \sum_j A_{ij}
\end{equation}

beziehungsweise

\begin{equation}
  D^{-1}_{ii} = \frac{1}{\sum_j A_{ij}}
\end{equation}

Matrixnormalisierung dann zum Beispiel $D^{-1}A$ oder $D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$.

Der erste Layer würde dann die Features eines Knotens mit denen der direkten Nachbarschaft kombinieren.
Ein weiterer Layer würde, diess für alle Wege mit Länge $2$ tun und so weiter.

Dies entspricht in etwa einer generaliserten Version des Weisfeiler-Lehman Algorithmus auf Graphen:

Für alle Knoten $v_i \in \mathcal{G}$:
\begin{enumerate}
  \item Sammle Merkmale $\lbrace h_{v_j} \rbrace$ für alle Nachbarschaftsknoten ${ v_j }$
  \item Update Knotenmerkmal $h_{v_i} \leftarrow \text{hash}(\sum_j h_{v_j})$
Wiederhole $k$-mal oder bis Konvergenz.
\end{enumerate}

Jedem Knoten wird also ein Merkmal zugeordnet, dass seine Rolle im Graphen beschreibt.
Dies funktioniert jedoch nicht gut für z.B. reguläre Graphen, bei dem jeder Knoten gleich viele Kanten besitzt.
Der Weisfeiler-Lehman Algorithmus wird oft benutzt, um Graphisomorphismen zu bestimmen.

\subsection{Klassifierung eines Graphen}

Wie kann dieses Model genutzt werden, um einen Graphen zu klassifizieren?
Wir erhalten $N \times F$ Features für einen Graphen nach $x$ vielen Convolutions.
$F$ kann natürlich dann auch die Anzahl der Klassen sein.

Weiteres Vorgehen:
Wir wollen nicht einzelne Knoten labeln, sondern das gesamte Bild bzw.\ den gesamten Graphen.
Diese Features beschreiben den Knoten sowie seine direkte und indirekte Nachbarschaft.
Die Knoten müssten jetzt eigentlich noch geeignet gelabelt werden und können dann in ein Netz gefüttert werden.

Wir können Spatial Informationen ausnutzen, wir wissen wie der Graph in der Lage aussieht.
Das sollte man sich ruhig zu nutze machen.

\section{Graph Pooling / Graph Coarsening / Graph Clustering}

Pooling Operationen erfordern es, Graphen basierend auf lokalen Nachbarschaften zu clustern.
Wenn dies pro Layer geschieht, dann entspricht dies einem Multi-Scale Clustering auf Graphen, der lokale geometrische Strukturen wahrt.
Graph Clustering ist jedoch NP-schwer, d.h.\ das insbesondere Approximationen benötigt werden, um effiziente Pooling Layer zu generieren.
Ein Graph Clustering Algorithmus, der die Knotenanzahl pro Layer um einen Faktor von $2$ reduziert, scheint für die Anwendung in einem CNN ideal, wie z.B. die \emph{Coarsening Phase} des \emph{Graclus Multilevel Clustering}-Algorithmus.

Der Algorithmus kriegt als Eingabe einen Graphen $\mathcal{G}_0 = (V_0, E_0, A_0)$ und die Anzahl an gewünschten Partitionen.
Die Coarsening Phase des Algorithmus reduziert den initialen Graphen $\mathcal{G}_0$ in immer kleinere Graphen $\mathcal{G}_1$, $\mathcal{G}_2$, \ldots, $\mathcal{G}_m$, sodass $|V_0| > |V_1| > |V_1| > \ldots > |V_m|$.
Um den Graphen zu vergröbern, werden Knoten aus $G_i$ zu einem Superknoten für den Graphen $G_{i+1}$ zusammengefasst.
Wenn eine Knotenauswahl kombiniert wird, dann werden die Kantengewichte als die Summe der Kantengewichte der originalen Knoten dargestellt.
Die Knotenmerkmale werden über Max Pooling berechnet.

Die Frage, die noch offen ist, ist wie eine Knotenauswahl zu einem Superpixel getroffen wird.
Der Ansatz, der hier verfolgt wird, heißt \emph{Heavy Edge Coarsening}.
Er funktioniert wie folgt:

\begin{enumerate}
  \item Sei ein Graph $G$ gegeben, dann setze alle seine Knoten auf den initialen unmarkierten Status.
  \item Durchlaufe alle Knoten in einer zufälligen Reihenfolge. Für jeden Knoten $x$:
  \begin{enumerate}
      \item \texttt{continue}, falls $x$ bereits markiert
      \item Verbinde $x$ mit dem unmarkierten Knoten $y$, sodass $y$ ein Nachbar von $x$ ist und das Kriterium zwischen $x$ und $y$ maximal für alle unmarkierten Nachbarn.
      \item Markiere $x$ und $y$.
      \item Falls es keinen Knoten $y$ gibt, d.h.\ alle Nachbarn von $x$ sind bereits markiert, markiere $x$ als alleinstehenden Knoten.
    \end{enumerate}
\end{enumerate}

Als Verbindungskriterium sind einige Freiheitsgrade gegeben:

\begin{itemize}
  \item das höchste Kantengewicht (vgl. Metis)
  \item \textbf{Max-cut coarsening}: maximiere $\frac{e(x, y)}{w(x)} + \frac{e(x, y)}{w(y)}$, wobei $w(x), w(y)$ die Gewichte der Knoten sind (z.B. Grad des Knoten (\textbf{Normalized Cut}))
\end{itemize}

Dieser Algorithmus reduziert die Knotenanzahl approximiert auf $2$.
Es kann natürlich ein paar nicht verbundene Knoten geben, sodass die Knotenanzahl ein bisschen weniger reduziert wird.

\section{Graph Labeling}

Jeder Ansatz von Graph hat zur Folge, dass wir irgendeine \emph{eindeutige} Sortierung von Knoten auf dem Graphen brauchen.
Wenn wir das nicht haben, dann haben zwei absolut gleiche Graphen mit unterschiedlicher Sortierung einen unterschiedlichen Output im Netz.
Das geht natürlich nicht.

Deswegen gibt es den \emph{Isomorphismus} und \emph{Automorphismus}.

In der Graphentheorie ist ein Isomorphismus zwischen zwei Graphen $G$ und $H$ eine Bijektion zwischen den Knoten der beiden Graphen $f: V(G) \rightarrow V(H)$, sodass genau dann zwei Knoten $u$ und $v$ in $G$ adjazent sind, wenn $f(u)$ und $(v)$ adjazent in $H$ sind.
Ein Automorphismus ist ein Isomorphismus von $G$ zu sich selbst.
Ein Automorphismus ist demnach eine Permutation der Knoten, der die Knoten und Kantenverbindungen aufrecht erhält.
Ein Automorphismus gibt demach eine Ordnung der Knoten an, die gleich ist für gleiche Graphen, die unterschiedlich gelabeled sind.

\subsection{Canonicalization}

\emph{Canonical Labeling} beschreibt den Prozess des Labelings eines Graphen $G$, so dass ein Graph der isomorph zu $G$ ist, das gleiche Labeling besitzt.

\section{Offene Fragen}

Adjazenzmatrix beschreibt den Abstand zu den einzelnen Knoten
Dabei sollen diese skalierungsinvariant sein und sich daher im Intervall $[0, 1]$ befinden.
Gewichte müssen dabei umgekehrt werden, d.h.\ eine $1$ beschreibt den nächstmöglichsten Abstand zu Knoten $v$.
Als ein Gewichtsmapping bietet sich dann die Funktion $f: x \rightarrow \frac{1}{x}$ an, wobei $\frac{1}{0} = 1$ für Self-Loops.
Damit sind wir immer noch skalierungsinvariant (glaube ich).
Gewichte $2$ und $4$ werden zu $0,5$ und $0,25$ gemappt mit Verhältnis $2$.
Gewichte $4$ und $8$ werden auf $0,25$ und $0,125$ gemappt mit Verhältnis $2$.

Wir definieren unsere Adjazenzmatrix $\tilde A$ aus $A$ dann wie folgt:

\begin{equation}
  \tilde A_{ij} = \begin{cases}
    1, & \text{wenn }i=j\text{,}\\
    1, & \text{wenn }0 < a_{ij} \leq 1\text{,}\\
    \frac{1}{a_{ij}}, & \text{wenn }a_{ij} > 1\text{,}\\
    0, & \text{sonst.}
  \end{cases}
\end{equation}

\todo{mathematische notation für matrixberechnung?}

Die diagonale Gradmatrix $\tilde D$ von $\tilde A$ ist dann definiert als $\tilde D_{ii} = \sum_j \tilde A_{ij}$.
Für eine Diagonalmatrix $D$ gilt dann:

\begin{equation}
  D^x = \begin{pmatrix}
    d_{11} & 0 & \cdots & 0\\
    0 & d_{22} & \cdots & 0\\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & d_{nn}\\
  \end{pmatrix}^x = \begin{pmatrix}
    d_{11}^x & 0 & \cdots & 0\\
    0 & d_{22}^x & \cdots & 0\\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & d_{nn}^x\\
  \end{pmatrix}
\end{equation}

\end{document}
