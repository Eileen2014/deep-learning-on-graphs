\section{Laufzeitanalyse}
\label{laufzeitanalyse}

Die Vorverarbeitung der Eingabedaten sowie das Training der neuronalen Netze erfolgte auf einem 3.5 GHz 6-Core Intel Xeon E5, 16GB DDR3 Arbeitsspeicher und einer Solid-State-Drive.
Die in dieser Arbeit präsentierten Laufzeiten wurden ausschließlich über Berechnungen auf dem Prozessor gewonnen.
Bei der Verwendung einer Grafikkarte stellen sich dabei Laufzeitgewinne in Bereichen, die auf die diese ausgelagert werden können, von $300 - 400 \%$ ein.

\paragraph{Vorverarbeitung}

Die Vorverarbeitung der Eingabedaten in eine geeignete Graphrepräsentation, das Berechnen der Merkmalsmatrix sowie zusätzliche Aufwände wie die Vergröberung des Graphen oder die Bestimmung der Receptive-Fields seiner Knoten, welche für die Eingabe in ein neuronales Netz notwendig sind, trägt einen entscheidenen Anteil an der Ausführungsgeschwindigkeit für das Training und die Auswertung eines neuronalen Netzes bei.
Graphen sollen demnach bestmöglichst zur Laufzeit eines Trainings generiert werden können und möglichst die Ausführung des eigentlichen Trainings garnicht bis kaum beeinflussen.

Abbildung~\ref{fig:laufzeit_raeumlich_pipeline} illustriert die relative Laufzeitverteilung der einzelnen räumlichen Vorverarbeitungsschritte \bzgl{} aller Datensätze und Superpixelalgorithmen.
\input{tikz/laufzeit_raeumlich_pipeline}
Dabei stellt sich die Berechnung der eindeutigen Receptive-Fields einer groß gewählten Kotenauswahl als Flaschenhals in (fast) allen Datensätzen heraus, die die meiste Zeit in Anspruch nimmt.
Lediglich auf großen Bildern, wie sie in \gls{Pascal} enthalten sind, zeigt sich der Aufwand der Segmentierung als ebenso kostspielig.

Abbildung~\ref{fig:laufzeit_spektral_pipeline} veranschaulicht auf ähnliche Weise die Laufzeitverteilung der spektralen Vorverarbeitungsschritte.
\input{tikz/laufzeit_spektral_pipeline}
Anstelle der Receptive-Field-Berechnung entsteht hier ein Mehraufwand durch die Berechnung der Graphvergröberung zu vier Ebenen.
Dieser Aufwand zeigt sich jedoch auf Bildern aus \gls{Pascal} als zu vernachlässigen, da die Laufzeit bei großen Bildern (fast) rein von der Laufzeit der Superpixelalgorithmen abhängig ist.

Tabelle~\ref{tab:laufzeit_raeumlich_spektral} stellt die absoluten Aufwände zur räumlichen sowie spektralen Vorverarbeitung eines Bildes aus allen Datensätze und Superpixelalgorithmen gegenüber.
\begin{table}[t]
\centering
\begin{tabular}{lrrrr}
  \toprule
  & \multicolumn{2}{c}{Räumlich [ms]} & \multicolumn{2}{c}{Spektral [ms]}\\
  \cmidrule{2-5}
  & \acs{SLIC} & \acs{QS} & \acs{SLIC} & \acs{QS}\\
  \midrule
  \acs{MNIST} & 30.59 & 24.00 & 20.32 & 19.40\\
  \acs{Cifar}-10 & 72.74 & 43.85 & 25.34 & 23.18\\
  \acs{Pascal} & 713.40 & 1080.78 & 372.79 & 782.56\\
  \bottomrule
\end{tabular}
\caption[Laufzeiten der räumlichen und spektralen Vorverarbeitung]{Laufzeiten der räumlichen sowie spektralen Vorverarbeitung für ein Bild der jeweiligen Datensätze und Superpixelalgorithmen basierend auf den Laufzeiten aus Abbildung~\ref{fig:laufzeit_raeumlich_pipeline} und~\ref{fig:laufzeit_spektral_pipeline}.
Die räumliche Vorverarbeitung ist aufgrund der einzelnen Receptive-Field-Berechnungen deutlich ineffizienter.}
\label{tab:laufzeit_raeumlich_spektral}
\end{table}
Wohingegen sich für Bilder mit kleinen Auflösungen aus den Datensätzen \gls{MNIST} und \gls{Cifar}-10 der Aufwand zur Vorverarbeitung in Grenzen hält, zeigen sich für ein Bild aus \gls{Pascal} die Kosten zur Vorverarbeitung als nicht zu unterschätzen.
Eine Vorverarbeitung zur Laufzeit ist auf diesem Datensatz daher (zurzeit) nicht zu empfehlen.
Weiterhin sei anzumerken, dass die spektralen Vorverarbeitungsschritte in jedem Versuchsaufbau entschieden schneller als die räumlichen berechnet werden können.

\paragraph{Training}

Tabelle~\ref{tab:train_laufzeit} zeigt die im Durschnitt ermittelten Laufzeiten des Trainings eines Batches der Größe $64$ für alle vorgestellten Netzarchitekturen.
\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lllrrrr}
  \toprule
  Datensatz & Superpixel & Verfahren & \multicolumn{4}{c}{Laufzeit [s]}\\
  \cmidrule{4-7}
   & & & vor Beginn & \multicolumn{3}{c}{zur Laufzeit}\\
  \cmidrule{5-7}
   & & & & Vorverarbeitung & Training & $\sum$\\
  \midrule
  \gls{MNIST} & \gls{SLIC} & \acs{RL} & 0.04 & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \gls{SLIC} & \acs{SGCNN} & & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \gls{SLIC} & \acs{GCN} & 0.20 & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \gls{SLIC} & \acs{EGCNN} & 0.61 & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \acs{QS}   & \acs{RL} & 0.04 & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \acs{QS}   & \acs{SGCNN} & 0.37 & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \acs{QS}   & \acs{GCN} & 0.21 & \multicolumn{3}{c}{—}\\
  \gls{MNIST} & \acs{QS}   & \acs{EGCNN} & 0.63 & \multicolumn{3}{c}{—}\\
  \midrule
  \gls{Cifar}-10 & \gls{SLIC} & \acs{SGCNN} & — & & & \\
  \gls{Cifar}-10 & \gls{SLIC} & \acs{GCN} &   — & 1.24 & 0.39 & 1.63 \\
  \gls{Cifar}-10 & \gls{SLIC} & \acs{EGCNN} & — & 0.44 & 2.00 & 2.44 \\
  \gls{Cifar}-10 & \acs{QS}   & \acs{SGCNN} & — & 0.43 & 1.95 & 2.38\\
  \gls{Cifar}-10 & \acs{QS}   & \acs{GCN} &   — & 1.09 & 0.36 & 1.45\\
  \gls{Cifar}-10 & \acs{QS}   & \acs{EGCNN} & — & & & \\
  \midrule
  \gls{Pascal} & \gls{SLIC} & \acs{EGCNN} & & \multicolumn{3}{c}{—}\\
  \gls{Pascal} & \acs{QS}   & \acs{EGCNN} & & \multicolumn{3}{c}{—}\\
  \midrule
  \gls{MNIST} & — & klassisch & & \multicolumn{3}{c}{—}\\
  \gls{Cifar}-10 & — & klassisch & 0.49 & \multicolumn{3}{c}{—}\\
  \gls{Pascal} & — & klassisch & & \multicolumn{3}{c}{—}\\
  \bottomrule
\end{tabular}}
\caption[Vergleich der Trainingslaufzeiten]{Vergleich der Laufzeiten des Trainings eines Batches der Größe 64 für alle vorgestellten Netzarchitekturen.}
\label{tab:train_laufzeit}
\end{table}
\gls{MNIST} \bzw{} \gls{Pascal} wurden aufgrund fehlender Augmentierung \bzw{} aufwändigen Berechnung vorab berechnnet, wohingegen die Grapheingabedaten für \gls{Cifar}-10 zur Laufzeit generiert wurden.
Es zeigt sich, dass das Training auf Graphen im Allgemeinen deutlich langsamer als im Vergleich zu ausgereiften Bildimplementierungen ist.
Lediglich der Ansatz des räumlichen Lernens kann aufgrund seiner Verwendung der klassischen Faltungsoperation und bei einer Vorverarbeitung vor Beginn des Trainings schnelle Laufzeiten garantieren.

Zwischen den spektralen Faltungsansätzen zeigt sich das \acs{GCN} aufgrund seiner geringen Berechnungskomplexität als sehr effizient, leidet jedoch wie in Kapitel~\ref{ergebnisse} beschrieben an mangelnder Genauigkeit.
Das \acs{EGCNN} weist unter allen Ansätzen die höchste Berechnungskomplexität auf.
Dies ist im Vergleich mit dem \acs{SGCNN} insoweit zu erklären, dass dessen Netzarchitektur in der Regel tiefer ist und mehrere Faltungsschichten hintereinander verlangt, um an Informationen nicht direkt benachbarter Knoten zu gelangen.
Das \acs{SGCNN} erfordert dies nicht, da dessen Filtergröße in direktem Zusammenhang zur Größe der Faltung steht.

Desweiteren zeigt sich in der Tabelle bei einer Vorverarbeitung während des Trainings, dass je höher die Laufzeit des eigentlichen Lernens ist, die Dauer der Vorverarbeitung umso geringer wird.
So zeigt sich \bspw{} bei \gls{Cifar}-10, dass das \acs{EGCNN} und \acs{SGCNN} in der Regel lediglich $\approx$ 0.4s auf die Vorverarbeitung warten muss, wohingegen bei einer effizienteren Faltung wie beim \acs{GCN} das Training pro Batch $\approx$ 1.1s pausiert.

\paragraph{Vergleich \bzgl{} vorhandener Implementierungen}
\label{vergleich_laufzeit}

\input{tikz/laufzeit_vergleich}
