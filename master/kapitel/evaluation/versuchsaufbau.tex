\section{Versuchsaufbau}
\label{versuchsaufbau}

Generelle Netzstruktur, softmax auf Klassen abgebildet
Conv mit MaxPool gefolgt von AveragePool auf Fully Connect auf Softmax

\subsection{Datensätze}
\label{datensaetze}

\paragraph{MNIST}
\label{MNIST}

Bei den Datensätzen insbesondere auch die Superpixelalgorithmen erwähnen
das heißt wie viele Superpixel
welche Parameter
mean/max Knotengrad
Wie viele Merkmale und welche?

\cite{mnist}

\paragraph{Cifar-10}
\label{cifar_10}

\cite{cifar_10}

\paragraph{Tiny ImageNet}
\label{tiny_image_net}

\cite{imagenet}

\paragraph{Pascal VOC}
\label{pascal_voc}

\cite{pascal_voc}

\subsection{Metriken}
\label{metriken}

\subsection{Parameterwahl}
\label{parameterwahl}

Vorstellung aller Parameter
Was gibt es denn hier überhaupt?
Dropout, L2 Regularisierung?
BatchSize?
Globale/normale Lokalisierung
Standardabweichung für Gauß

Alle Faltungen wurden dabei mit einer Partitionsgröße von $8$ bei $K=0$ und $K=1$ implementiert, um ein \gls{CNN} mit einem $3 \times 3$ Filter zu simulieren.
Es erscheint jedoch vorstellbar die Filtergröße bei größerer lokaler Kontrollierbarkeit, \dhe{} $K > 1$, weiter zu reduzieren und die Gefahr des Overfittings damit aufgrund der kleineren Anzahl an Trainingsparametern einzuschränken.

\paragraph{Datenreduktion}
\label{datenreduktion}

\paragraph{Augmentierung von Graphen}
\label{augmentierung_von_graphen}

hier auf die Formeln von TensorFlow referenzieren, d.h. TensorFlow Quelle angeben
\cite{tensorflow}

Augmentierung auf Graphen über left/right
Farbanpassungen


nesser ist es, dass Bild vorher zu ändern, da sich dadurch die Superpixelrepräsentation ändert
und folglich zu realisiterischer Augmentierung führt.

\paragraph{Vorverarbeitung und Eingabe der Daten}
\label{vorverarbeitung}
