\section{Ergebnisse}
\label{ergebnisse}

Dieses Kapitel stellt die Ergebnisse einer ausgewählten Untermenge der in dieser Arbeit vorgestellten Graphrepräsentationen von Bildern, den Ansätzen zum Lernen auf diesen sowie den beschriebenen Datensätzen und Superpixelalgorithmen vor.
Zur Kenntlichmachung der einzelnen Ansätze erhält das räumliche Lernen auf Graphen dabei die Abkürzung \enquote{\acs{RL}} und das spektrale Lernen auf Graphen über den Tschebyschow-Polynomen \bzw{} dessen Einschränkung auf eine Filtergröße von Eins die Abkürzungen \enquote{\acs{SGCNN}} \bzw{} \enquote{\acs{GCN}}.
Der erweiterte Ansatz zum spektralen Lernen ist über \enquote{\acs{EGCNN}} gekennzeichnet.

Netzarchitekturen werden im Folgenden \bspw{} über ein Muster der Form \enquote{C32-P2-FC128} beschrieben und signalisieren eine Faltungsschicht, die auf 32 Merkmalskarten abbildet, gefolgt von einer Poolingschicht, welche zwei Knoten vereint, und danach mit einer vollverbundenen Schicht zu 128 Neuronen verknüpft wird.
Die für das spektrale Lernen bei dynamischen Eingabegrößen notwendige Durchschnittsbildung zwischen Faltungs- und vollverbundener Schicht wird als \enquote{Avg} gekennzeichnet (\vgl{} Kapitel~\ref{spektrale_netzarchitektur}).
Auf die explizite Erwähnung der Eingabe- und Ausgabeschichten der Netze wird übersichtshalber verzichtet.

\paragraph{\gls{MNIST}}

Für die Validierung der Faltungsansätze starten wir mit dem \gls{MNIST}-Datensatz und der in Kapitel~\ref{gitter} beschriebenen Graphrepräsentation der Bilder über ein reguläres Gitter.
Damit kann jedes Bild über den selben Graphen dargestellt werden.
Insbesondere verfällt damit die Notwendigkeit der Durchschnittsbildung, da die Anzahl an Neuronen zu jedem Zeitpunkt klar definiert ist.
Für das räumliche Lernen wurden 196 Knoten mit Schrittweite 4 bei $\delta=1$ und einer Nachbarschaftsgröße von 25 besimmt (\vgl{} Kapitel~\ref{raeumliches_lernen}).
Die Graphvergröberung des spektralen Lernens wiederum generiert für die erste Schicht 976 Knoten ($28^2 = 784$ Pixelknoten und 192 Fakeknoten).
Die Anzahl der generierten Fakeknoten kann jedoch aufgrund der zufälligen Permutation variieren.
Tabelle~\ref{tab:train_mnist_gitter} fasst die Netzarchitekturen und dessen erreichte Genauigkeiten der einzelnen Ansätze zusammen.
\begin{table}[t]
\centering
\begin{tabular}{lclrr}
  \toprule
  Ansatz & \ma{W} & Architektur & Genauigkeit [\%]\\
  \midrule
  \acs{RL} & 25 & C64-FC1024 & \\
  \acs{SGCNN} & 25 & C32-P4-C64-P4-FC1024 & 98.888\\
  \acs{GCN} & 1 & C32-C32-P4-C64-C64-P4-FC1024 & 96.675\\
  \acs{EGCNN} & 9 & C32-C32-P4-C64-C64-P4-FC1024 & 99.145\\
  \midrule
  klassisch & $5 \times 5$ & C32-P4-C64-P4-FC1024 & 99.189\\
  klassisch & $3 \times 3$ & C32-C32-P4-C64-C64-P4-FC1024 & 99.139\\
  \bottomrule
\end{tabular}
\caption[Testgenauigkeiten der \gls{MNIST}-Gitterrepräsentation]{Testgenauigkeiten eines Trainings auf einer \gls{MNIST}-Gitterrepräsentation nach 10 Epochen für die vorgestellten Ansätze inklusive äquivalenter Netzarchitekturen bei Benutzung der klassischen Faltungsoperation.
Die Spalte \ma{W} kennzeichnet die benutzten Filtergrößen der einzelnen Faltungsschichten.}
\label{tab:train_mnist_gitter}
\end{table}
Die Netzarchitekturen sind dabei an der von \texttt{tensorflow} vorgeschlagenen Architektur orientiert~\cite{tensorflow}.
Alle Netze wurden einheitlich mit einer Lernrate von $0.001$ trainiert.
Es wurde auf die für \gls{MNIST} eher untypische Augmentierung der Eingabedaten, die L2-Regularisierung sowie auf die Verminderung der Lernrate in Abhängigkeit zur Trainingsdauer verzichet~\cite{tensorflow}.
Es findet sich jedoch die Anwendung eines Dropouts von $0.5$ vor der Ausgabeschicht.
\citeauthor{Defferrard} benutzen zur Validierung des \acs{SGCNN}s eine ähnliche Netzarchitektur auf \gls{MNIST} und schlagen dazu eine Filtergröße von $25$ vor, sodass die Anzahl der Parameter mit der Anzahl der klassischen Faltung über ein $5 \times 5$ Fenster korrespondiert~\cite{Defferrard}.
Diese Faltung lässt sich im Kontext des spektralen Faltungsoperators auf einem Gitter der Größe $28 \times 28$ aber kaum noch als lokale Faltung verstehen, denn sie lässt Knoten in die Berechnung mit einfließen, die 25 Kanten weit vom Ursprungsknoten entfernt liegen, und betrachtet folglich außer bei Randknoten das globale Bild (\vgl{} Kapitel~\ref{spektraler_faltungsoperator}).
Eine lokalere Faltung lässt sich mit dem \acs{RL}, \acs{GCN} und \acs{EGCNN} gewinnen.
Für das \acs{GCN} zeigt sich dabei jedoch eine eindeutige Limitierung durch die geringe Filtergröße.
Die entwickelten Faltungsansätze \acs{RL} und \acs{EGCNN} beweisen sich aufgrund der Berücksichtigung ihrer Kantenausrichtungen als die bessere Alternative.
Insbesondere lässt sich die Äquivalenz des \acs{EGCNN}s \bzgl{} regulärer Gitter verifizieren.
Das räumliche Lernen zeigt sich aufgrund ihrer eindimensionalen Knotenauswahl und uneinheitlichen Nachbarschaftsanordnung an Randknoten als leicht schwächer.

Die in Tabelle~\ref{tab:train_mnist} vorgestellten Netzarchitekturen zeigen die Genauigkeiten des Trainings auf irregulären Dateneingaben, welche über die Superpixelalgorithmen \gls{SLIC} und \gls{QS} anhand der in Kapitel~\ref{datensaetze} vorgestellten Parameterwerte gewonnen wurden.
\begin{table}[t]
\centering
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \acs{SLIC} & \acs{QS}\\
  \midrule
  \acs{RL} & 25 & C64-F1600-F256 & 82.873 & 85.226 \\
  \acs{SGCNN} & 9 & C64-P4-C128-P4-Avg & 91.884 & 92.718 \\
  \acs{GCN} & 1 & C64-C64-P4-C128-C128-P4-Avg & 78.155 & 86.358 \\
  \acs{EGCNN} & 9 & C64-C64-P4-C128-C128-P4-Avg & 97.405 & 98.025 \\
  \bottomrule
\end{tabular}
\caption[Testgenauigkeiten der \gls{MNIST} Superpixelrepräsentationen]{Testgenauigkeiten eines Trainings auf dem \gls{MNIST} Datensatz nach 17 Epochen, dessen Bilder zuvor durch einen Superpixelalgorithmus in irreguläre Graphrepräsentationen gebracht werden.}
\label{tab:train_mnist}
\end{table}
Für \acs{SGCNN} wurde dabei aufgrund der kleineren Eingabemenge die Filtergröße im Vergleich zum Vortest gedrosselt und besitzt im Vergleich zu den \acs{GCN}- und \acs{EGCNN}-Netzarchitekturen nicht mehrere, aufeinanderfolgende Faltungsschichten.
Es zeigt sich jedoch, dass alle Netzansätze auf irregulären Daten garnicht oder nur knapp an die Ergebnisse auf regulären Gittereingaben heranreichen.
Insbesondere offenbart der räumliche Faltungsansatz hier seine Schwächen, denn obwohl die Trainingsmenge erfolgreich gelernt wird, so lassen sich unbekannte Eingaben aufgrund der Anordnung zu eindimensionalen Knoten- sowie Nachbarschaftsmengen für irreguläre Graphen nur schwer bis garnicht generalisieren.
Für alle Ansätze zeigt sich desweiteren die Quickshift-Segmentierung aufgrund ihrer überlegenden Segmentierung auf den Bildern des \gls{MNIST} Datensatz als die bessere Wahl (\vgl{} Abbildung~\ref{fig:mnist}).
Insbesondere erreicht der entwickelte spektrale Faltungsoperator über B-Spline-Kurven (\acs{EGCNN}) auch hier die eindeutig besten Resultate.

\paragraph{\gls{Cifar}-10}

Die Klassifikation auf dem \gls{Cifar}-10 Datensatz ist im Verlgeich zu \gls{MNIST} eine schwierigere Angelegenheit, die eine weitaus längere Trainingsdauer in Anspruch nimmt.

% Abbildung~\ref{fig:cifar_10_train} zeigt
\input{tikz/cifar_10_train}

% Postprocessing weitaus besser als preprocessing!

LearningRate: 0.001
LearningRateDecay: Alle 5000 Steps um 0.96
BatchSize: 64
WeightDecay in FC256 und FC128: 0.004
Dropout vor letztem: 0.5
jeweils 65k Steps (entspricht ungefähr 75 Epochen)
65k steps trainiert: vereinheitlichen wir zu 60k

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \gls{SLIC} & \acs{QS}\\
  \midrule
  \acs{SGCNN} & 9 & C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & & 67.748\\
  \acs{GCN} & 1 & C64-C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & 54.497 & 53.606\\
  \acs{EGCNN} & 9 & C64-C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & 74.218 & 75.230\\
  \midrule
  klassisch & $3 \times 3$ & C64-C64-P4-C128-P4-C256-P4-FC256-FC128 & \multicolumn{2}{c}{82.061} \\
  \bottomrule
\end{tabular}}
\caption[Testgenauigkeiten der \gls{Cifar}-10 Superpixelrepräsentationen]{65k steps}
\label{tab:train_cifar_10}
\end{table}

\paragraph{\gls{Pascal}}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \gls{SLIC} & \acs{QS}\\
  \midrule
  \acs{EGCNN} & 9 & &\\
  \midrule
  klassisch & — & SqueezeNet 1.1 & \multicolumn{2}{c}{54.731} \\
  \bottomrule
\end{tabular}}
\caption[Testgenauigkeiten der \gls{Pascal} Superpixelrepräsentationen]{50k Steps}
\label{tab:train_pascal}
\end{table}

PascalVOC Squeeze einheitliche Größe von $224 \times 224$.
Squeeze vorstellen
insbesondere nur Vorverarbeitung vorm Training (aufgrund Laufzeiten)

% Alle Faltungen wurden dabei mit einer Partitionsgröße von $8$ bei $K=0$ und $K=1$ implementiert, um ein \gls{CNN} mit einem $3 \times 3$ Filter zu simulieren.
% Es erscheint jedoch vorstellbar die Filtergröße bei größerer lokaler Kontrollierbarkeit, \dhe{} $K > 1$, weiter zu reduzieren und die Gefahr des Overfittings damit aufgrund der kleineren Anzahl an Trainingsparametern einzuschränken.
