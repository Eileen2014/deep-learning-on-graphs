\section{Ergebnisse}
\label{ergebnisse}

Dieses Kapitel stellt die Ergebnisse aller Datensätze über allen Verfahren, \dhe{} dem \gls{RL}, dem \gls{SGCNN} über den Tschebyschow-Polynomen, den Graph Convoluional Networks (GCN) und dem angepassten Ansatz \gls{EGCNN}

über den Superpixelalgorithmen SLIC und \gls{QS}.

% Alle Faltungen wurden dabei mit einer Partitionsgröße von $8$ bei $K=0$ und $K=1$ implementiert, um ein \gls{CNN} mit einem $3 \times 3$ Filter zu simulieren.
% Es erscheint jedoch vorstellbar die Filtergröße bei größerer lokaler Kontrollierbarkeit, \dhe{} $K > 1$, weiter zu reduzieren und die Gefahr des Overfittings damit aufgrund der kleineren Anzahl an Trainingsparametern einzuschränken.

\paragraph{\gls{MNIST}}

keine Augmentierung
keine L2-Regularisierung
kein Learning Rate Decay

10.000 Steps mit Batch Size 64\\
Learning-Rate: 0.001\\
Dropout: 0.5\\

\begin{table}[t]
\centering
\begin{tabular}{lclrr}
  \toprule
  Ansatz & \ma{W} & Architektur & Genauigkeit [\%]\\
  \midrule
  \gls{RL} & 25 & C64-F2048-FC1024 & \\
  \gls{SGCNN} & 25 & C32-P4-C64-P4-FC1024 & 98.888\\
  \gls{GCN} & 1 & C32-C32-P4-C64-C64-P4-FC1024 & 96.675\\
  \gls{EGCNN} & 9 & C32-C32-P4-C64-C64-P4-FC1024 & 99.145\\
  \midrule
  klassisch & $5 \times 5$ & C32-P4-C64-P4-FC1024 & 99.189\\
  klassisch & $3 \times 3$ & C32-C32-P4-C64-C64-P4-FC1024 & 99.139\\
  \bottomrule
\end{tabular}
\caption[Testgenauigkeiten der \gls{MNIST}-Gitterrepräsentation]{Testgenauigkeiten eines Trainings auf einer \gls{MNIST}-Gitterrepräsentation nach 10000 Durchläufen ($\approx 11$ Epochen) für die vorgestellten Ansätze inklusive äquivalenter Netzarchitekturen bei Benutzung der klassischen Faltungsoperation.}
\label{tab:train_mnist_gitter}
\end{table}

kein preprocessing nötig.
es wird immer die gleiche Adjazenzmatrix benutzt.
nur zum Validieren
Es zeigt sich dass \gls{EGCNN} wirklich äquivalent zu der Faltung auf $3 \times 3$ ist.
insbesondere kein average pool notwendig, da graphen alle gleich groß

\begin{table}[t]
\centering
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \gls{SLIC} & \gls{QS}\\
  \midrule
  \gls{RL} & 25 & C64-F1600-F256 & & \\
  \gls{SGCNN} & 9 & C64-P4-C128-P4-Avg-F128 & & \\
  \gls{GCN} & 1 & C64-C64-P4-C128-C128-P4-Avg-F128 & & \\
  \gls{EGCNN} & 9 & C64-C64-P4-C128-C128-P4-Avg-F128 & & \\
  \bottomrule
\end{tabular}
\caption[Testgenauigkeiten der \gls{MNIST} Superpixelrepräsentationen]{10k steps}
\label{tab:train_mnist}
\end{table}

\paragraph{\gls{Cifar}-10}

% Abbildung~\ref{fig:cifar_10_train} zeigt
\input{tikz/cifar_10_train}

% Postprocessing weitaus besser als preprocessing!

LearningRate: 0.001
LearningRateDecay: Alle 5000 Steps um 0.96
BatchSize: 64
WeightDecay in FC256 und FC128: 0.004
Dropout vor letztem: 0.5
jeweils 65k Steps (entspricht ungefähr 75 Epochen)
65k steps trainiert: vereinheitlichen wir zu 60k

Chebyshev: Es werden pro Faltung eine 8fache Nachbarschaft betrachtet.
Das ist riesig!

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \gls{SLIC} & \gls{QS}\\
  \midrule
  \gls{RL} & 25 & C64-FC1024-FC512-FC128 & & \\
  \gls{SGCNN} & 9 & C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & & 67.748\\
  \gls{GCN} & 1 & C64-C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & & 53.606\\
  \gls{EGCNN} & 9 & C64-C64-P2-C128-P2-C256-P2-C512-Avg-FC256-FC128 & & 75.230\\
  \midrule
  klassisch & $3 \times 3$ & C64-C64-P4-C128-P4-C256-P4-FC256-FC128 & \multicolumn{2}{c}{82.061} \\
  \bottomrule
\end{tabular}}
\caption[Testgenauigkeiten der \gls{Cifar}-10 Superpixelrepräsentationen]{65k steps}
\label{tab:train_cifar_10}
\end{table}

\paragraph{\gls{Pascal}}

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lclrrr}
  \toprule
  Ansatz & \ma{W} & Architektur & \multicolumn{2}{c}{Genauigkeit [\%]}\\
  \cmidrule{4-5}
   & & & \gls{SLIC} & \gls{QS}\\
  \midrule
  \gls{EGCNN} & 9 & &\\
  \midrule
  klassisch & — & SqueezeNet 1.1 & \multicolumn{2}{c}{54.731} \\
  \bottomrule
\end{tabular}}
\caption[Testgenauigkeiten der \gls{Pascal} Superpixelrepräsentationen]{50k Steps}
\label{tab:train_pascal}
\end{table}

PascalVOC Squeeze einheitliche Größe von $224 \times 224$.
Squeeze vorstellen
insbesondere nur Vorverarbeitung vorm Training (aufgrund Laufzeiten)
