\section{Schwächen}

\begin{itemize}
  \item TensorFlow hat keine gute Anbindung an dynamischen Input (den wir hier aber brauchen)
  \item Augmentierung nicht mehr einfach möglich
  \begin{itemize}
    \item Graphen können nicht ohne weiteres gedreht werden
    \item Augmentierung der Form-Features oder der Farbe verändert auch den Graphen
    \item Graphgenerierung im Postprocessing schwierig, weil langsam
  \end{itemize}
\item Jeder Superpixelalgorithmus und jeder Datensatz (verschiedene Bildauflösungen/anders Anwendungsgebiet) erfordert eigentlich Neuberechnung der geeignesteten Form Features
  \item PCA nicht möglich, weil zu teuer (alle Form Features zu berechnen ist utopisch und wiederspricht der Grundidee)
  \item viel langsamer als ausgereifte Bildimplementierungen
  \item abhängig von der Anzahl der Form-Features und der Größe des Graphen nicht unbedingt speichereffizienter als ein einzelnes Bild, wir haben aber in der Tat kleinere Convolutions.
  \item es gibt Superpixelalgorithmen, die bei einer Menge von Bildern unterschiedliche Parameter brauchen für geeignete Superpixelrepräsentationen.
    Diese Algorithmen sind zum Lernen ungeeignet (z.B.\ Felzenszwalb)
\end{itemize}

\subsection{Übergang zum FC-Layer}

Der Übergang zum Fully-Connected Layer kann durchaus als eine Schwäche im Vergleich zu klassischen CNN auf Bildern angesehen werden.
Pixel und daraus entstehende Receptive-Fields haben auch im späteren Verlauf der Layer eine Ordnung.
Diese Ordnung ist auf Graphen und auch auf eingebetten Graphen nicht oder nur bedingt gegeben und man kann diese nicht ausnutzen. (siehe Email)
Die Lösung für dieses Problem besteht darin, so viele Layer vor dem FC-Layer zu stacken, sodass die Receptive-Field-Größe sehr klein ist und als Merkmale für das gesamte Bild aufgefasst werden können.
