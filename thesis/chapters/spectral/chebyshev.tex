\subsection{Tschebyschow-Polynome}

\begin{itemize}
  \item Faltung ist sehr teuer aufgrund der Berechnungen von $\gls{Lboth}^k$
  \item \underline{Idee:} beschreibe $\hat g^{\prime}\left(\gls{Lboth}\right)$ als eine polynomielle Funktion, die rekursiv aus $\gls{Lboth}$ berechnet werden kann.
  \item Warum sollte das effizienter sein? $\Rightarrow$ $\gls{Lboth}$ ist nicht dicht besetzt, und hat nur $\left|\gls{E}\right| + n \ll n^2$ Einträge mit $n \leq \left|\gls{E}\right|$
\end{itemize}

\emph{Tschebyschow-Polynome} (engl. \emph{Chebyshev}) bezeichnen eine Menge von Polynomen $\gls{T}_n\left(x\right) \colon \gls{R} \to \gls{R}$ mit dem rekursiven Zusammenhang
\begin{equation}
  \gls{T}_n\left(x\right) = 2x \gls{T}_{n-1}\left(x\right) - \gls{T}_{n-2}\left(x\right)
\end{equation}
mit $\gls{T}_0\left(x\right) = 1$ und $\gls{T}_1\left(x\right) = x$.
Ein Tschebyschow-Polynom $\gls{T}_n$ ist ein Polynom $n$-ten Grads.

Für $x \in \left[-1, 1\right]$ gilt $\gls{T}_k\left(x\right) \in \left[-1, 1\right]$

Rescale $\gls{Lambda}$ zu $\gls{tLambda} = \frac{2}{\gls{lambda}_{\max}} \gls{Lambda} - \gls{I} \in {\left[-1, 1\right]}^{n \times n}$.\todo{warum nochmal?}

Dann ist $\gls{T}_k\left(\gls{tLambda}\right) \in {\left[-1, 1\right]}^{n \times n}$

\begin{equation}
  \hat g^{\prime}\left(\gls{Lambda}\right) = \sum_{i=0}^k c_i \gls{Lambda}^i = \sum_{i = 0}^k c^{\prime}_i \gls{T}_i \left(\gls{tLambda}\right)
\end{equation}

Analog lässt sich wiederum $\hat g^{\prime}\left(\gls{Lboth}\right)$ definieren mit
\begin{equation}
  \hat g^{\prime}\left(\gls{Lboth}\right) = \sum_{i=0}^k c^{\prime}_i \gls{T}_i\left(\gls{tLboth}\right)
\end{equation}
mit $\gls{tLboth} = \gls{Eiv}\gls{tLambda}\gls{Eiv}^{\top} = \frac{2}{\gls{lambda}_{\max}} \gls{Lboth} - \gls{I}$.

Jetzt lässt sich $\ve{f}_{\mathrm{out}} = \hat g^{\prime}\left(\gls{Lboth}\right) \ve{f}_{\mathrm{in}} = \sum_{i=0}^k c^{\prime}_i \gls{T}_i \left(\gls{tLboth}\right) \ve{f}_{\mathrm{in}}$ sehr schnell berechnen:

\begin{enumerate}
  \item berechnne $\ve{\overline{f}}_i := \gls{T}_i \left(\gls{tLboth}\right) \ve{f}_{\mathrm{in}}$ für alle $i \in \left\{ 0, 1, \ldots, k \right\}$ mit Hilfe von Rekursion:
  \begin{enumerate}
    \item $\ve{\overline{f}}_0 = \ve{f}_{\mathrm{in}}$
    \item $\ve{\overline{f}}_1 = \gls{tLboth} \ve{f}_{\mathrm{in}}$
    \item $\ve{\overline{f}}_i = 2\gls{tLboth} \ve{\overline{f}}_{i-1} - \ve{\overline{f}}_{i-2}$
  \end{enumerate}
\item berechne $\ve{f}_{\mathrm{out}} = \left[\ve{\overline{f}}_0, \ve{\overline{f}}_1, \ldots, \ve{\overline{f}}_{k} \right] \ve{c}^{\prime}$, wobei $\ve{c}^{\prime} = \left[c^{\prime}_0, \ldots, c^{\prime}_k\right] \in \gls{R}^{k+1}$
\end{enumerate}

\subsubsection{Laufzeit}

 \begin{itemize}
   \item anstatt $\gls{Lboth}^k$ zu berechnen mit Komplexität $\gls{O}\left(n^2\right)$ haben wir nur noch $k$ Matrix-Vektor-Multiplikationen mit der Matrix $\gls{tLboth}$
   \item da $\gls{Lboth}$ für große Graphen sehr dünnbesetzt ist, d.h.\ $\left|\gls{E}\right| \ll n^2$, haben wir bei Verwendung von \emph{dünnbesetzten Matrizen} nur noch eine Laufzeit von $\gls{O}\left(k\left|\gls{E}\right|\right)$~\cite{Hammond, Defferrard}
   \item für den zweiten Schritt gilt $\gls{O}\left(kn\right)$, mit $n \leq \left|\gls{E}\right|$ bedingt damit nur Schritt Eins die Laufzeit
 \end{itemize}
