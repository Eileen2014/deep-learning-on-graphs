\chapter{Einleitung}
\section{Motiviation}
\section{Aufbau der Arbeit}

\chapter{Grundlagen}
\section{Mathematische Notationen}
Vektoren, Matrizen und Tensoren
insbesondere dünnbesetzte Matrizen
\section{Graphentheorie}
\section{Neuronale Netze}
\subsection{Convolutional Neural Networks}
Was ist ein CNN\@?
Wie ist der Faltungsoperator definiert?
Was ist ein Filter?
Was ist eine Filtergröße?
Aktivierungsfunktionen?
Dynamische Eingabegröße? (evtl.\ erst später)

\chapter{Bildrepräsentationen durch Graphen}
\section{Eingebettete Graphen}
\subsection{Kantengewichte}
\section{Gitter}
\section{Superpixel}
\subsection{Verfahren}
\subsubsection{SLIC}
\subsubsection{Quickshift}
\subsubsection{Weitere Verfahren}
\subsection{Adjazenzmatrixbestimmung}
\subsection{Merkmalsextraktion}

\chapter{Stand der Forschung}

\chapter{Räumliches Lernen auf Graphen}
\section{Räumliche Graphentheorie}
\subsection{Färbung von Knoten}
\subsection{Isomorphie und kanonische Ordnung}
\section{Räumliche Faltung}
\subsection{Knotenauswahl}
\subsection{Nachbarschaftsgruppierung}
\subsection{Normalisierung}
\section{Erweiterung auf eingebettete Graphen}
\section{Netzarchitektur}

\chapter{Spektrales Lernen auf Graphen}
\section{Spektrale Graphentheorie}
\subsection{Eigenwerte und Eigenvektoren reell symmetrischer Matrizen}
\subsection{Laplace-Matrix}
\subsubsection{Visuelle Interpretation}
\subsubsection{Eigenschaften}
\section{Spektraler Faltungsoperator}
\subsection{Graph-Fourier-Transformation}
\subsection{Polynomielle Approximation}
\subsubsection{Tschebyschow-Polynome}
\section{Graph Convolutional Networks}
\subsection{Faltungsoperator}
\subsection{Weisfeiler-Lehman Analogie}
\subsection{Erweiterung auf eingebettete Graphen}
\subsubsection{B-Spline-Kurven}
\subsubsection{Faltungsoperator}
\section{Pooling auf Graphen}
\subsection{Clustering von Knoten}
\subsubsection{Normalized Cut}
\subsection{Vergröberung von Graphen}
\subsection{Erweiterung auf eingebettete Graphen}
\section{Netzarchitektur}

\chapter{Implementierung}
\section{Verwendete Technologien}
\section{Vorverarbeitungsschritt}

\chapter{Evaluation}
\section{Datensätze}
\subsection{MNIST}
\subsection{Cifar10}
\subsection{PascalVOC}
\section{Metriken}
\section{Parameterwahl}
Vorstellung aller Parameter
Superpixelalgorithmen Parameterwahl
\section{Merkmalsselektion}
\newpage
\section{Ergebnisse}
\subsection{MNIST}
\subsubsection{Gitter}

10.000 Steps mit Bach Size 64\\
Learning-Rate: 0.001\\
Dropout: 0.5\\

\begin{tabular}{p{2.5cm}p{7.5cm}cc}
  \hline
  Verfahren & Netzarchitektur & Dauer & Genauigkeit\\
  \hline
  klassisch & C5-32-P4-C5-64-P4-FC1024 & 0.18s & 99.189\\
  klassisch & C32-C32-P4-C64-C64-P4-FC1024 & 0.25s & 99.139\\
  Chebyshev & C25-32-P4-C25-64-P4-FC1024 & 0.91s & 98.888\\
  GCN & C32-C32-P4-C64-C64-P4-FC1024 & 0.22s & 96.675\\
  E-GCN & C32-C32-P4-C64-C64-P4-FC1024 & 0.77s & 99.145\\
  \hline
\end{tabular}

\subsubsection{Superpixel}

\begin{tabular}{p{2.5cm}p{7.5cm}cc}
  \hline
  Verfahren & Netzarchitektur & Dauer & Genauigkeit\\
  \hline
  SLIC E-GCN & C32-P2-C64-P2-C128-P2-C256-P2-FC128 & 1.12s & 96.124\\
  Quickshift E-GCN & C32-P2-C64-P2-C128-P2-C256-P2-FC128 & 1.35s & 97.656\\
  \hline
\end{tabular}

\subsection{Cifar10}
\subsection{PascalVOC}

Learning-Rate: 0.1\\
Dropout: 0.5\\
nur 2000 Steps trainiert!\\

\begin{tabular}{p{2.5cm}p{7.5cm}cc}
  \hline
  Verfahren & Netzarchitektur & Dauer & Genauigkeit\\
  \hline
  SLIC E-GCN & C32-C32-P2-C64-C64-P2-C128-C128-P2-C256-C256-P2-C512-C512-P2-FC256-FC128 & 29s & 60.546\\
  \hline
\end{tabular}

\newpage

  \subsection{Vergleich mit anderen Implementierungen}
\section{Laufzeitanalyse}
  \subsection{Vergleich mit anderen Implementierungen}
\section{Räumlicher Ansatz vs.\ spektraler Ansatz}
Schwächen und Vorteile beider Ansätze

\chapter{Zusammenfassung}

\chapter{Ausblick}
\section{Weitere Anwendungsgebiete}
z.B. Segmentierung
\section{Augmentierung von Graphen}
\section{Spatial-Pyramid-Pooling}
\section{Attention-Algorithmus}

Grundlagen:
Graphen, insbesondere planare Graphen
Mathematische Notationen: Vektor, Matrix, Tensor
Neuronale Netze (Was ist ein CNN, wie ist der Convolution Operator definiert, nicht lineare Aktivierungsfunktion)
Faltung, insbesondere Faltung im CNN

Graphrepräsentationen von Bildern
Grid
Superpixel
Superpixelalgorithmen
Umwandlung von Kanten von Distanz zu Gauß
Merkmalextraktion (Momente)
Merkmalselektion (Cov, PCA)

Lernen auf Graphen:
Stand der Forschung: Spatial vs Spectral

Spatial:
Patchy
Zentralität
Canonical Labeling
Übertragung auf planare Graphen <- EIGENER ANTEIL (z.B. Grid Spiral)
Komplexität
Vorteile (einfache Architektur)/Nachteile (keine direkte Nachbarschaftsberücksichtigung möglich,

keine Graph Coarsening möglich, Vorverarbeitung ist recht teuer und muss Preprocessed werden weil man das nicht über Matrixoperationen ausdrücken kann)

Spectral:
Laplacian, Fourier Transformation
GCN und kGCN (weisfeiler Lehman)
Übertragung auf planare Graphen (Adjazenzpartitionierung) <- EIGENER ANTEIL
Pooling/Coarsening
Komplexität
Vorteile (z.B. Nachbarschaftsberücksichtigung/keine Ordnung nötig)/Nachteile (rotationsinvariant)

Deep Learning auf variabler Input-Menge (SPP)

Augmentierung von Graphen (ist das überhaupt möglich)
COARSENING IST RANDOM (EINE FORM DER AUGMENTIERUNG)
PERMUTATE RANDOM
REMOVE RANDOM EDGE

Realisierung (Experimente) und Evaluation
Adam-Optimizer
Sparse Tensors
Vorstellung Datensätze (MNIST, PascalVOC, CIFAR-10, ImageNet)
Tensorflow
Dropout L2-Regularisierung

Zusammenfassung und Ausblick
